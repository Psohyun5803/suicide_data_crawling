import json
import pandas as pd
from collectors.common import replace_latest_dated_file
from utils.file_utils import ensure_parent_dir
from utils.metadata import update_meta

METADATA_PATH = "../data/metadata/metadata.json"

def find_min_data(metadata_path = METADATA_PATH): # ê°±ì‹ ì´ ê°€ì¥ ì˜¤ë˜ì „ì— ëœ ë‚ ì§œ ì°¾ê¸° 
    with open(metadata_path, "r", encoding="utf-8") as f:
        meta = json.load(f)
    max_dates =[]
    for k,v in meta.items():
        if "max_date" in v and v["max_date"]:
            max_dates.append(pd.to_datetime(v["max_date"],format="%Y-%m",errors="raise"))
    common_min_date = min(max_dates)
    #print("ê¸°ì¤€ ì›”",common_min_date)
    return common_min_date 

def load_and_trim_monthly(csv_path, start_date, end_date, date_col="date"):
    df = pd.read_csv(csv_path)

    # 1) ë‚ ì§œ â†’ Timestamp (í˜¼í•© í¬ë§· ì•ˆì „)
    df[date_col] = pd.to_datetime(df[date_col], format="mixed", errors="raise")

    start_date = pd.to_datetime(start_date, format="mixed")
    end_date = pd.to_datetime(end_date, format="mixed")

    # 2) ê¸°ê°„ í•„í„°ë§ (Timestampë¼ë¦¬ ë¹„êµ)
    df = df[(df[date_col] >= start_date) & (df[date_col] <= end_date)]

    # 3) ìµœì¢… í¬ë§· í†µì¼ (ë¬¸ìì—´ë¡œ ë°”ê¾¸ëŠ” ê±´ ë§ˆì§€ë§‰ì—)
    df[date_col] = df[date_col].dt.strftime("%Y-%m")

    return df.sort_values(date_col).reset_index(drop=True)
    """
    df = pd.read_csv(csv_path)
    # ë‚ ì§œ íŒŒì‹± (ì›” í¬ë§· ê³ ì •)
    df[date_col] = pd.to_datetime(df[date_col], format="%Y-%m", errors="raise")

    start_date = pd.to_datetime(start_date, format="%Y-%m")
    end_date = pd.to_datetime(end_date, format="%Y-%m")

    df = df[(df[date_col] >= start_date) & (df[date_col] <= end_date)]
    return df.sort_values(date_col).reset_index(drop=True)
    """

def merge_all_monthly_from_metadata(metadata_path = METADATA_PATH, start_date ="2020-01", date_col="date"):
    with open(metadata_path, "r", encoding="utf-8") as f:
        meta = json.load(f)

    dfs = []
    common_min_date = find_min_data()
    for key, info in meta.items():
        if key == "suicide_base_data":
            continue
        csv_path = info["saved_file"]

        df = load_and_trim_monthly(
            csv_path=csv_path,
            start_date=start_date,
            end_date=common_min_date,
            date_col=date_col
        ) # start ~ end ê¸°ê°„ìœ¼ë¡œ ë°ì´í„°ë¥¼ ë‹¤ ìë¥¸ë‹¤ 
        dfs.append(df)

    # inner join: ê³µí†µ ê¸°ê°„ë§Œ ë‚¨ê¹€
    df_merged = dfs[0]
    for df in dfs[1:]:
        df_merged = df_merged.merge(df, on=date_col, how="inner")

    return df_merged.sort_values(date_col).reset_index(drop=True)

def run(cfg:dict):
    start_date = cfg.get("start_date", "2020-01")
    output_csv_tpl = cfg["output_csv"]              # "../data/suicide_base_data_2020_{max_year}.csv"
    metadata_key = cfg.get("metadata_key", "suicide_base_data")

    df = merge_all_monthly_from_metadata(start_date=start_date)

    max_year = pd.to_datetime(df["date"]).dt.year.max()

    # ğŸ”¥ íŒŒì¼ëª… ë™ì  ì¹˜í™˜
    out_csv = output_csv_tpl.format(max_year=max_year)

    # 3) ì €ì¥ (ê¸°ì¡´ collector ìŠ¤íƒ€ì¼ ê·¸ëŒ€ë¡œ)
    ensure_parent_dir(out_csv)
    out_csv = replace_latest_dated_file(out_csv)
    df.to_csv(out_csv, index=False, encoding="utf-8-sig")

    # 4) metadata ê¸°ë¡ (ğŸ”¥ Timestamp â†’ ë¬¸ìì—´ ë³€í™˜)
    max_date_str = (
        pd.to_datetime(df["date"]).max().strftime("%Y-%m")
        if not df.empty else None
    )

    update_meta(METADATA_PATH, metadata_key, {
        "saved_file": out_csv,
        "rows": int(df.shape[0]),
        "max_date": max_date_str,     # âœ… JSON ì§ë ¬í™” ì•ˆì „
        "start_date": start_date,     # ë¬¸ìì—´  
    })

    print(
        "âœ… Suicide_Base_Data ì €ì¥:",
        out_csv,
        "rows:", len(df),
        "max_date:", max_date_str,
    )
    """
    df = merge_all_monthly_from_metadata()

    max_year = df["date"].max().year
    df.to_csv(f"../data/suicide_base_data_2020_{max_year}.csv", index=False, encoding="utf-8-sig")
    print ( "âœ… Suicide_Base_Data ì €ì¥" )
    """
    
